{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "ABGPm5uuyNYK",
        "outputId": "e30061dc-381f-4c57-d8b7-c397fc721a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'MultipleFiles/final.csv' not found. Please ensure the file path is correct.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1983221823.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# --- Preprocessing ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# Drop the unnamed index column if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv('/content/final.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'MultipleFiles/final.csv' not found. Please ensure the file path is correct.\")\n",
        "    exit()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "df.drop_duplicates(inplace=True)\n",
        "# Drop the unnamed index column if it exists\n",
        "if df.columns[0] == 'Unnamed: 0':\n",
        "    df = df.drop(columns=df.columns[0])\n",
        "\n",
        "# Convert 'Airport with 50km Radius' and 'DSLR Allowed' to numerical (1/0)\n",
        "df['Airport with 50km Radius'] = df['Airport with 50km Radius'].map({'Yes': 1, 'No': 0})\n",
        "df['DSLR Allowed'] = df['DSLR Allowed'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Handle 'Establishment Year'\n",
        "# Convert 'Unknown' to NaN, then convert to numeric, then handle negative years\n",
        "df['Establishment Year'] = df['Establishment Year'].replace('Unknown', np.nan)\n",
        "df['Establishment Year'] = pd.to_numeric(df['Establishment Year'], errors='coerce')\n",
        "# Replace negative years with NaN (assuming they are data entry errors or historical anomalies not useful for numerical comparison)\n",
        "df.loc[df['Establishment Year'] < 0, 'Establishment Year'] = np.nan\n",
        "\n",
        "# Define features (X) and target (y) for model training\n",
        "# We'll use 'Type' as the target for classification\n",
        "X_model = df.drop(columns=['Name', 'Type']) # 'Name' is unique identifier, 'Type' is target\n",
        "y_model = df['Type']\n",
        "\n",
        "# --- Correction for ValueError: Handle rare classes in y_model ---\n",
        "# This step ensures that 'stratify' in train_test_split can work correctly by removing classes\n",
        "# that have only one member, as they cannot be split into both train and test sets.\n",
        "\n",
        "# Count occurrences of each class in the target variable\n",
        "class_counts = y_model.value_counts()\n",
        "\n",
        "# Identify classes (Types) that appear only once\n",
        "rare_classes = class_counts[class_counts == 1].index.tolist()\n",
        "\n",
        "if rare_classes:\n",
        "    print(f\"Warning: The following 'Type' classes have only one member and will be removed for model training (due to stratification requirement): {rare_classes}\")\n",
        "    # Create a boolean mask to keep only rows where 'Type' is NOT in the rare_classes list\n",
        "    rows_to_keep = ~y_model.isin(rare_classes)\n",
        "\n",
        "    # Apply the mask to both X_model and y_model\n",
        "    X_model = X_model[rows_to_keep].copy() # .copy() to avoid SettingWithCopyWarning\n",
        "    y_model = y_model[rows_to_keep].copy() # .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "    # Reset index after filtering to ensure clean indices\n",
        "    X_model.reset_index(drop=True, inplace=True)\n",
        "    y_model.reset_index(drop=True, inplace=True)\n",
        "else:\n",
        "    print(\"No rare classes (single members) found in 'Type' for model training.\")\n",
        "\n",
        "# Identify numerical and categorical features for preprocessing\n",
        "numerical_features = ['Establishment Year', 'time needed to visit in hrs', 'Google review rating',\n",
        "                      'Entrance Fee in INR', 'Number of google review in lakhs']\n",
        "categorical_features = ['Zone', 'State', 'City', 'Weekly Off', 'Significance', 'Best Time to visit']\n",
        "binary_features = ['Airport with 50km Radius', 'DSLR Allowed'] # Already handled, but keep for completeness if not mapped\n",
        "\n",
        "# Ensure all numerical columns are numeric, coercing errors to NaN\n",
        "for col in numerical_features:\n",
        "    X_model[col] = pd.to_numeric(X_model[col], errors='coerce')\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "# Numerical: Impute missing values with mean, then scale\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# --- CORRECTION: Set sparse_output=False in OneHotEncoder ---\n",
        "# This forces the OneHotEncoder to return a dense array, which is compatible\n",
        "# with all KNN distance metrics.\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # <--- ADDED THIS\n",
        "])\n",
        "\n",
        "# Combine transformers using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features + binary_features), # Include binary features here\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep other columns if any, though none expected here\n",
        ")\n",
        "\n",
        "# Encode the target variable 'Type'\n",
        "label_encoder = LabelEncoder()\n",
        "y_model_encoded = label_encoder.fit_transform(y_model)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# This should now work correctly as single-member classes have been removed\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_model, y_model_encoded, test_size=0.2, random_state=42, stratify=y_model_encoded)\n",
        "\n",
        "# --- Hybrid Model Implementation ---\n",
        "\n",
        "# Define different distance metrics for KNN\n",
        "knn_metrics = {\n",
        "    'Euclidean': 'euclidean',\n",
        "    'Manhattan': 'manhattan',\n",
        "    'Cosine': 'cosine',\n",
        "    'Chebyshev': 'chebyshev',\n",
        "    'Minkowski (p=3)': 'minkowski' # Minkowski with p=3\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "# K-Fold Cross-Validation setup\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"--- Building and Evaluating Hybrid KNN+SVM Models ---\")\n",
        "\n",
        "for metric_name, metric_func in knn_metrics.items():\n",
        "    print(f\"\\n--- KNN Metric: {metric_name} ---\")\n",
        "\n",
        "    # Create a pipeline for KNN\n",
        "    knn_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('knn', KNeighborsClassifier(n_neighbors=5, metric=metric_func)) # Using n_neighbors=5 as a starting point\n",
        "    ])\n",
        "\n",
        "    # Evaluate KNN using cross-validation\n",
        "    knn_cv_scores = cross_val_score(knn_pipeline, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "    print(f\"KNN (Accuracy) Cross-Validation Scores: {knn_cv_scores}\")\n",
        "    print(f\"KNN (Accuracy) Mean CV Score: {np.mean(knn_cv_scores):.4f}\")\n",
        "\n",
        "    # Train KNN on the full training data to get predictions for the test set\n",
        "    knn_pipeline.fit(X_train, y_train)\n",
        "    knn_test_predictions = knn_pipeline.predict(X_test)\n",
        "    print(f\"KNN (Accuracy) on Test Set: {accuracy_score(y_test, knn_test_predictions):.4f}\")\n",
        "    print(f\"KNN (F1-Score) on Test Set: {f1_score(y_test, knn_test_predictions, average='weighted'):.4f}\")\n",
        "    # print(f\"KNN Classification Report:\\n{classification_report(y_test, knn_test_predictions, target_names=label_encoder.classes_)}\")\n",
        "\n",
        "    # --- SVM Component ---\n",
        "    # Create a pipeline for SVM\n",
        "    svm_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('svm', SVC(kernel='rbf', random_state=42, probability=True)) # RBF kernel often performs well, enable probability for future use\n",
        "    ])\n",
        "\n",
        "    # Evaluate SVM using cross-validation\n",
        "    svm_cv_scores = cross_val_score(svm_pipeline, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "    print(f\"SVM (Accuracy) Cross-Validation Scores: {svm_cv_scores}\")\n",
        "    print(f\"SVM (Accuracy) Mean CV Score: {np.mean(svm_cv_scores):.4f}\")\n",
        "\n",
        "    # Train SVM on the full training data\n",
        "    svm_pipeline.fit(X_train, y_train)\n",
        "    svm_test_predictions = svm_pipeline.predict(X_test)\n",
        "    print(f\"SVM (Accuracy) on Test Set: {accuracy_score(y_test, svm_test_predictions):.4f}\")\n",
        "    print(f\"SVM (F1-Score) on Test Set: {f1_score(y_test, svm_test_predictions, average='weighted'):.4f}\")\n",
        "    # print(f\"SVM Classification Report:\\n{classification_report(y_test, svm_test_predictions, target_names=label_encoder.classes_)}\")\n",
        "\n",
        "    results[metric_name] = {\n",
        "        'KNN_Accuracy_CV_Mean': np.mean(knn_cv_scores),\n",
        "        'KNN_Accuracy_Test': accuracy_score(y_test, knn_test_predictions),\n",
        "        'KNN_F1_Test': f1_score(y_test, knn_test_predictions, average='weighted'),\n",
        "        'SVM_Accuracy_CV_Mean': np.mean(svm_cv_scores),\n",
        "        'SVM_Accuracy_Test': accuracy_score(y_test, svm_test_predictions),\n",
        "        'SVM_F1_Test': f1_score(y_test, svm_test_predictions, average='weighted')\n",
        "    }\n",
        "\n",
        "print(\"\\n--- Summary of Model Performance ---\")\n",
        "for metric, scores in results.items():\n",
        "    print(f\"\\nKNN Metric: {metric}\")\n",
        "    print(f\"  KNN Mean CV Accuracy: {scores['KNN_Accuracy_CV_Mean']:.4f}\")\n",
        "    print(f\"  KNN Test Accuracy: {scores['KNN_Accuracy_Test']:.4f}\")\n",
        "    print(f\"  KNN Test F1-Score: {scores['KNN_F1_Test']:.4f}\")\n",
        "    print(f\"  SVM Mean CV Accuracy: {scores['SVM_Accuracy_CV_Mean']:.4f}\")\n",
        "    print(f\"  SVM Test Accuracy: {scores['SVM_Accuracy_Test']:.4f}\")\n",
        "    print(f\"  SVM Test F1-Score: {scores['SVM_F1_Test']:.4f}\")\n",
        "\n",
        "\n",
        "# --- Interactive Recommendation System ---\n",
        "\n",
        "print(\"\\n--- Travel Destination Recommendation System ---\")\n",
        "\n",
        "# Get unique values for user choices from the original df for full range of options\n",
        "# (even if some 'Type' categories were removed for model training, the original df\n",
        "# still contains all places for recommendation filtering)\n",
        "available_zones = df['Zone'].unique()\n",
        "available_significances = df['Significance'].unique()\n",
        "\n",
        "print(\"\\nAvailable Zones:\", \", \".join(available_zones))\n",
        "print(\"Available Significances:\", \", \".join(available_significances))\n",
        "\n",
        "while True:\n",
        "    user_zone = input(\"\\nEnter your preferred Zone (e.g., Northern, Southern, Eastern, Western): \").strip()\n",
        "    user_significance = input(\"Enter your preferred Significance (e.g., Historical, Religious, Nature, Recreational): \").strip()\n",
        "\n",
        "    # Validate user input\n",
        "    if user_zone not in available_zones:\n",
        "        print(f\"Invalid Zone: '{user_zone}'. Please choose from the available zones.\")\n",
        "        continue\n",
        "    if user_significance not in available_significances:\n",
        "        print(f\"Invalid Significance: '{user_significance}'. Please choose from the available significances.\")\n",
        "        continue\n",
        "\n",
        "    # Filter places based on user input from the original dataframe (df)\n",
        "    filtered_places = df[(df['Zone'] == user_zone) & (df['Significance'] == user_significance)]\n",
        "\n",
        "    if filtered_places.empty:\n",
        "        print(f\"\\nNo places found for Zone: '{user_zone}' and Significance: '{user_significance}'.\")\n",
        "    else:\n",
        "        # --- Correction for potential non-numeric values in sorting columns ---\n",
        "        # Convert 'Google review rating' and 'Number of google review in lakhs' to numeric,\n",
        "        # coercing errors to NaN, then fill NaNs with 0 for sorting purposes.\n",
        "        # Using .loc to avoid SettingWithCopyWarning\n",
        "        filtered_places.loc[:, 'Google review rating'] = pd.to_numeric(filtered_places['Google review rating'], errors='coerce')\n",
        "        filtered_places.loc[:, 'Number of google review in lakhs'] = pd.to_numeric(filtered_places['Number of google review in lakhs'], errors='coerce')\n",
        "\n",
        "        # Fill NaNs in sorting columns with a value that won't disrupt sorting (e.g., 0)\n",
        "        # Using .loc to avoid SettingWithCopyWarning\n",
        "        filtered_places.loc[:, 'Google review rating'].fillna(0, inplace=True)\n",
        "        filtered_places.loc[:, 'Number of google review in lakhs'].fillna(0, inplace=True)\n",
        "\n",
        "        # Sort by Google review rating (descending) and then by Number of google review in lakhs (descending)\n",
        "        recommended_places = filtered_places.sort_values(\n",
        "            by=['Google review rating', 'Number of google review in lakhs'],\n",
        "            ascending=[False, False]\n",
        "        )\n",
        "\n",
        "        print(f\"\\n--- Top Recommendations for Zone: '{user_zone}' and Significance: '{user_significance}' ---\")\n",
        "        # Display relevant columns for recommendation\n",
        "        display_cols = ['Name', 'City', 'State', 'Type', 'Google review rating', 'Number of google review in lakhs', 'Entrance Fee in INR', 'time needed to visit in hrs']\n",
        "        print(recommended_places[display_cols].head(10).to_string(index=False)) # Display top 10 without index\n",
        "\n",
        "    another_recommendation = input(\"\\nDo you want another recommendation? (yes/no): \").strip().lower()\n",
        "    if another_recommendation != 'yes':\n",
        "        print(\"Thank you for using the recommendation system!\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y33kiL9zyVds"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}